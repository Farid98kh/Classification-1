####################
# Farid Khorshidi  #
#------------------#
#------------------#
#     Denetimli    #
#   Ä°statistiksel  #
#      Ã–ÄŸrenme     #
#------------------#

library(readr)
library(corrplot)
library(caret)
library(tidyverse)
library(magrittr)
library(olsrr)
library("neuralnet")
library(car)
library(corrplot)
library(nortest)
library(ISLR)
library(Hmisc)
library(caret)
library(dplyr)
library(ModelMetrics)
library(lmtest)
library(moments)
library(bestNormalize) # normalizasyon 
library(MASS)
library(psych) 
library(mvnTest) #multivariate normality test kutuphanesi
library(tree) #regresyon ve karar agaclari kutuphanesi
library(randomForest) # random forest kutuphanesi
library(rpart)       # regresyon agaclari
library(rpart.plot)  # plotting regression trees
library(ipred)       # bagging
library(klaR)
library(rpart.plot)
library("mice")
library(ggplot2)
library(devtools)
library(ggord)
library(heplots)
library(pROC)
library(rlang)



# heart veri setini 'kmed' paketinden "get" fonksyonu ile cagiriyoruz
df <- get(data("heart", package= "kmed"))
str(df)

#heart veri setini analiz etmek iÃ§in
#sÄ±nÄ±flandÄ±rma aÄŸaÃ§larÄ±nÄ± kullanarak baÅŸlayacaÄŸÄ±z.
#Bu verilerde, class kesikli ama 4 duzeyli bir deÄŸiÅŸkendir ve bu nedenle onu
#binary bir deÄŸiÅŸkene dÃ¶nÃ¼ÅŸtÃ¼rerek baÅŸlÄ±yoruz.
#bunun icin 0 degeri alan class verileri icin 0
# ve 0 olmayanlar icin 1 atadik.
df$class[df$class == 0 ] <- 0
df$class[df$class != 0 ] <- 1
df$sex[df$sex == FALSE] <- 0
df$fbs[df$fbs == FALSE] <- 0
df$exang[df$exang == FALSE] <- 0


# bir siniflandirma problemi oldugu icin numeric deger turunde olan degiskenleri
# degistirmiyoruz ama onun disinda diger veri turunde olan ve kategorik degiskenleri bagimli degisken
# ile birlikte factor turune ceviriyoruz
df$sex <- as.factor(df$sex)
df$fbs <- as.factor(df$fbs)
df$exang <- as.factor(df$exang)
df$class <- as.factor(df$class)

# toplamda 14 tane sutundan ve 297 tane veriden kayip veri olmaksizin
# olusmus bagimli degiskenle birlikte 8 tane factor ve kesikli
# ve 6 tane numeric degisken gorulmektedir.
# ozet istatistiklerine bakarak her bir degiskeni yorumlayabiliriz
str(df)
dim(df)
sum(is.na(df))
summary(df)

#Veriler Ã¼zerindeki bir sÄ±nÄ±flandÄ±rma probleminin performansÄ±nÄ±
#dÃ¼zgÃ¼n bir ÅŸekilde deÄŸerlendirmek iÃ§in, ilk Ã¶nce gÃ¶zlemleri bir 
#eÄŸitim seti ve bir test seti olarak ayÄ±rdÄ±k:
smp_size <- floor(0.70 * nrow(df))
set.seed(2022900191) 
train_ind <- sample(nrow(df), size = smp_size, replace = FALSE)
train <- df[train_ind, ]
test <- df[-train_ind, ]
str(train)
str(test)



#############################################################################################################################################################################



###################################################
#-------------------------------------------------#
#             Classification tree                 #
#-------------------------------------------------#
###################################################
#tÃ¼m deÄŸiÅŸkenleri kullanarak class'i tahmin etmek iÃ§in
# tree() fonksyonunu kullanÄ±yoruz
treeclass <- tree(class~. ,  train )
summary(treeclass )

classification = rpart(class~.,method = "class", data = train)
rpart.plot(classification,type = 1,extra= 3,
        fallen.leaves= FALSE,clip.facs= FALSE,
        faclen= 0, cex = 0.5,xlim= c(0, 1),ylim= c(0, 1))

# error rate Ã¶nemli:yanlis siniflandirma hatasi
# Toplam 15 terminal node yani uc dugum ile aÄŸaÃ§ oluÅŸturulmuÅŸ.
#aÄŸaÃ§ta dahili dÃ¼ÄŸÃ¼mler (karar noktalarÄ±nÄ± oluÅŸturan)
# olarak kullanÄ±lan deÄŸiÅŸkenler sayisi 9 tane dir
# Residual mean deviance 0.5209 olarak dikkat Ã§ekiyor.
# EÄŸitim hata oranÄ±nÄ±n yani Error rate 0.1256 gibi normal
#sayÄ±labilecek bir deÄŸer aldigini goruyoruz.
# SÄ±nÄ±flandÄ±rma aÄŸaÃ§larÄ± iÃ§in, summary() Ã§Ä±ktÄ±sÄ±nda bildirilen sapma ÅŸu ÅŸekilde verilir:
# science smith***


#AÄŸaÃ§ yapÄ±sÄ±nÄ± gÃ¶rÃ¼ntÃ¼lemek iÃ§in plot() 
#fonksyonunu ve dÃ¼ÄŸÃ¼m etiketlerini gÃ¶rÃ¼ntÃ¼lemek iÃ§in text() fonksyonunu kullanÄ±rÄ±z.
plot(treeclass )
text(treeclass ,pretty =0)
# Ä°lk dugum kategorik (3)normal , (6)fixed defect, (7)reversable defect
# ayÄ±rdÄ±ÄŸÄ±ndan, kalp hastaligi olmasinda en Ã¶nemli gÃ¶stergesi olarak gÃ¶rÃ¼nmektedir.
#thal: 3 root node olarak saptanmistir.


tree_pred <- predict(treeclass, test, type = "class")
table(tree_pred, test$class)
#aÄŸacÄ±n test verileri Ã¼zerindeki performansÄ±nÄ±
#deÄŸerlendirdigimizde Bu yaklaÅŸÄ±m, test veri setinin yaklaÅŸÄ±k
#(42+29)/90 = %79'si iÃ§in doÄŸru tahminlere yol aÃ§ar.

###########################################################
##############          Budama               ##############
###########################################################
# aÄŸacÄ± budamanÄ±n daha iyi sonuÃ§lara yol aÃ§Ä±p
# aÃ§mayacaÄŸÄ±na bakacagiz.
# aÄŸaÃ§ karmaÅŸÄ±klÄ±ÄŸÄ±nÄ±n en uygun dÃ¼zeyini belirlemek
#iÃ§in cross valdition gerÃ§ekleÅŸtirir
cv.treeclass <- cv.tree(treeclass, FUN = prune.misclass)
cv.treeclass
par(mfrow=c(1,2))
plot(cv.treeclass$size ,cv.treeclass$dev ,type="b")
plot(cv.treeclass$k ,cv.treeclass$dev ,type="b")
# deviance alaninin cross valdiation hata oranina 
# karsilik geldigini biliyoruz
# 6 dugumlu agaca en az hata orani 41'e karsilik gelmis
# grafikten de bunu gore biliriz

# Her iki grafik de incelendiÄŸinde size'Ä±n 6 olduÄŸu noktada deviance'de belirgin azalmalar dikkat Ã§ekmekte.
# Bu sebeple size 6 olarak seÃ§ilerek budamaya devam edilecektir.

#simdi 13 dugumden olusan agaci best=6 ile budama 
prune.treeclass <- prune.misclass (treeclass,best=6)
summary(prune.treeclass)
dev.off()
plot(prune.treeclass )
text(prune.treeclass ,pretty =0)
# aÄŸaÃ§ta dahili dÃ¼ÄŸÃ¼mler (karar noktalarÄ±nÄ± oluÅŸturan)
# olarak kullanÄ±lan deÄŸiÅŸkenler sayisi 4 tane dir.
# Budama sonrasÄ± yalnÄ±zca 6 node ile model residual mean deviance'Ä±nÄ±n 0.8011 olarak bir artÄ±ÅŸ gÃ¶stermiÅŸ gÃ¶rÃ¼nÃ¼yor.
# Error rate'de de artÄ±ÅŸ gÃ¶rÃ¼lmekte.
# terminal dugum azaldigi icin cok verimli bir agac oldugu soylenilemez.

################################
### Budama oncesi  Tahminler ###
#-------------------------------

#######              #############
####### train verisi #############
#######              #############
classtree.pred <- predict(treeclass ,train ,type="class")
caret::confusionMatrix(classtree.pred, train$class)
# Accuracy Rate :0.8744 
# Sensitivity : 0.9375  
# Specificity :0.8000  


#######              #############
####### test  verisi #############
#######              #############
classtree.predtest <- predict(treeclass, test, type = "class")
caret::confusionMatrix(classtree.predtest, test$class)

# Accuracy Rate : 0.7889 
# Sensitivity :0.8750   
# Specificity :0.6905 



################################
### Budama sonrasi Tahminler ###
#-------------------------------

##################################
####### train verisi #############
#######              #############
prunedtree.pred <- predict(prune.treeclass ,train ,type="class")
caret::confusionMatrix(prunedtree.pred, train$class)
# Accuracy Rate :0.8599
# Sensitivity : 0.8929 
# Specificity :0.8211  


#######              #############
####### test  verisi #############
#######              #############
prunedtree.predtest <- predict(prune.treeclass, test, type = "class")
caret::confusionMatrix(prunedtree.predtest, test$class)
# Accuracy Rate : 0.8  
# Sensitivity :0.8542
# Specificity :0.7381   

# sadece Test verisinde budanmadan sonra aciklayicilik biraz artmis
# train verisinde aciklayicilik azalmistir
# sensitivity hem test ve hem trainde budanmadan sonra azalmis.

# Budama sonrasÄ± sÄ±nÄ±flandÄ±rmanÄ±n performansÄ±nÄ±n, ama test verisinin accuracy rate degerinin az artisi
#disinda dÃ¼ÅŸtÃ¼ÄŸÃ¼nÃ¼ accuracy rate, sensivity ve specificity deÄŸerleri incelenerek kolaylÄ±kla gÃ¶zlemlenebilir.



################################################################################################################################################



###################################################
#-------------------------------------------------#
#                    Bagging                      #
#-------------------------------------------------#
###################################################

bag <- randomForest(class~. ,data=train, mtry=13,importance=TRUE)

bag$importance
varImpPlot(bag)

#######              #############
####### train verisi #############
#######              #############

baggintrain <- predict(bag ,train ,type="class")

caret::confusionMatrix(baggintrain, train$class)


#######              #############
####### test  verisi #############
#######              #############

baggintest <- predict(bag, test, type = "class")
caret::confusionMatrix(baggintest, test$class)



#############################################################################################################################################################################



###################################################
#-------------------------------------------------#
#               Random Forest                     #
#-------------------------------------------------#
###################################################

rf <- randomForest(class~. ,data=train, mtry=4,importance=TRUE)
plot(rf)

rf$importance
varImpPlot(rf)



rf_pred <- predict(rf ,train ,type="class")

caret::confusionMatrix(rf_pred, train$class)




rf_tahmin <- predict(rf, test, type = "class")

caret::confusionMatrix(rf_tahmin, test$class)


################################################################################################################################################



###################################################
#-------------------------------------------------#
#             Logistic Regression                 #
#-------------------------------------------------#
###################################################

logmodel1 <- glm(class ~ age + sex + cp + trestbps + chol +
        fbs + restecg + thalach + exang + oldpeak + slope +
        ca + thal , data = train, family = binomial)

summary(logmodel1)
coefficients(logmodel1)
residuals(logmodel1)
plot(logmodel1)



# KatsayÄ±lar lojistik Regresyonda olasÄ±lÄ±k deÄŸerini doÄŸrusal olarak etkilemez.
# BaÄŸÄ±msÄ±z deÄŸiÅŸkenin deÄŸerini bir birim arttÄ±rdÄ±ÄŸÄ±mÄ±zda tahmin deÄŸerindeki deÄŸiÅŸikliÄŸi
# belirlemek iÃ§in Ã¶nce log(odds) formulÃ¼nde her iki tarafa exp fonksiyonu uygulanÄ±r.
# Ã–nemli Nokta burada farklarÄ±na bakmÄ±yoruz iki tahminin oranÄ±na bakÄ±yor


#######    Anlamli   #############
####### degiskenlerin ############
#######    yorumu    #############

exp(1.811705)
# sexTRUE deÄŸiÅŸkenindeki 1 birimlik artis, odds oranini 6.12 kat degistirir.

exp(1.818234)
#cp4 deÄŸiÅŸkenindeki 1 birimlik artis, odds oranini 6.16 kat degistirir.

exp(0.026729)
#trestbps deÄŸiÅŸkenindeki 1 birimlik artis, odds oranini 1.027 kat degistirir.


exp(1.080424)
#ca deÄŸiÅŸkenindeki 1 birimlik artis, odds oranini 2.94 kat degistirir.

exp(1.250900)
# thal7 deÄŸiÅŸkenindeki 1 birimlik artis, odds oranini 3.49 kat degistirir.


confint.default(logmodel1)

# katsayÄ±sÄ±na ait %95 lik gÃ¼ven aralÄ±ÄŸÄ± sÄ±fÄ±r deÄŸerini kapsadÄ±ÄŸÄ± iÃ§in H0 hipotezi red edilemeyerek aÅŸaÄŸÄ±daki deÄŸiÅŸkenlerin
# class degiskenine etkisinin istatistiksel olarak anlamlÄ± olmadÄ±ÄŸÄ±na karar verilir.
# age , cp2 , cp3 , chol , fbsTRUE , restecg1 , restecg2 , thalach , exangTRUE , 
# oldpeak , slope2 , slope3 , thal6


# odds iÃ§in gÃ¼ven aralÄ±ÄŸÄ±
# EÄŸer odds oran deÄŸerine ait gÃ¼ven aralÄ±ÄŸÄ± 1 deÄŸerini iÃ§ermiyor ise H0 hipotezi red edilerek ilgili katsayÄ±nÄ±n istatistiksel olarak anlamlÄ± olduÄŸuna karar verilir.
odds.confint <- exp(confint.default(logmodel1))
odds.confint
str(df)
# cinsiyeti erkek olan bir kisinin class 1 olma oddsu %95 gÃ¼venle cinsiyeti kadin olanin 1.63 ile 22.97 katÄ± arasÄ±nda deÄŸer alÄ±r.
# cp4 asymptomatic bir birim bÃ¼yÃ¼k olan bir kiÅŸinin cp1 e gore class 1 yani kalp hastaligi olma oddsu %95 gÃ¼venle 1.16 katÄ± ile 32.62 katÄ± arasÄ±nda deÄŸer alÄ±r.
# trestbps istrahat kan basinci bir birim artarsa bir birimin dusugune gore, class 1 yani kalp hastaligi olma oddsu %95 gÃ¼venle 1 ile 1.05 katÄ± arasÄ±nda deÄŸer alÄ±r.
# chol bir birim artarsa bir birim dusugune gore, class 1 yani kalp hastaligi olma oddsu %95 gÃ¼venle 1 ile 1.05 katÄ± arasÄ±nda deÄŸer alÄ±r.
# ca bir birim artarsa bir birim dusugune gore, class 1 yani kalp hastaligi olma oddsu %95 gÃ¼venle 1.59 ile 5.43 katÄ± arasÄ±nda deÄŸer alÄ±r.
# thal7 bir birim buyuk olan bir kisinin thal3 e gore, class 1 yani kalp hastaligi olmaoddsu %95 guvenle 1.30 ile 9.37 arasinda deger alir.

## modelin anlamliligi

#ð» 0 : ð›½ 1 = ð›½ 2 = â‹¯ = ð›½ ð‘˜ = 0
# ð» 1 : ð¸ð‘› ð‘Žð‘§ðš¤ð‘›ð‘‘ð‘Žð‘› ð‘ð‘–ð‘Ÿ ð›½ ð‘— â‰  0

# G= Null deviance-Residual Deviance

285.57 - 133.00
206 - 188


1-pchisq(38.46, 18)
# p deÄŸeri 0.00336523 oldu ve 0.5'dan kÃ¼Ã§Ã¼k Ã§Ä±kmÄ±ÅŸtÄ±r.
# H 0 :Î² AGE = 0 hipotezi red edilir.
# BaÄŸÄ±msiz deÄŸiÅŸkenlerin class deÄŸiÅŸkenini aÃ§Ä±klamada etkili olduÄŸunu sÃ¶yleyebilecek yeterli istatistiksel kanÄ±tÄ±mÄ±z bulunmaktadÄ±r.


vif(logmodel1)
# herhangi bir baÄŸlantÄ± problemi gÃ¶rÃ¼lmemektedir.

# SPECIFICITY AND SENSITIVITY

# Ã‡alÄ±ÅŸÄ±lan bu siniflandirma Ã¶rneÄŸe gÃ¶re threshold degerini ozet istatistiklerine dayali ikisinden birini seÃ§ip
# onu mÃ¼mkÃ¼n olduÄŸunca yÃ¼kseltmek doÄŸru seÃ§enektir.

#######              #############
####### train verisi #############
#######     1        #############

pred_log1 <- fitted(logmodel1)
summary(pred_log1)
threshold <- 0.357706

# threshold atamasini tanimliyoruz.
# threshold degeri ppred uyumlu degerden az olursa 0 fazla olursa 1 atariz
pred_log1[pred_log1 > threshold] <- 1
pred_log1[pred_log1 < threshold] <- 0

sensitivity(pred_log1, train$class)
specificity(pred_log1, train$class)
# biraz artÄ±ÅŸ olsa da yine de istediÄŸim oranÄ± saÄŸlayamadÄ±m

#Confusion Matrix

caret::confusionMatrix(as.factor(pred_log1), train$class)

#######              #############
####### test  verisi #############
#######      1       #############
pred_log = predict(logmodel1, newdata=test, type='response')
summary(pred_log)
threshold <- 0.357706
pred_log[pred_log > threshold] <- 1
pred_log[pred_log < threshold] <- 0

sensitivity(pred_log, test$class)
specificity(pred_log, test$class)

# Confusion Matrix
caret::confusionMatrix(as.factor(pred_log), test$class)


#######              #############
####### train verisi #############
#######     2        #############
pred_log1 <- fitted(logmodel1)
summary(pred_log1)
threshold <- 0.458937
pred_log1[pred_log1 > threshold] <- 1
pred_log1[pred_log1 < threshold] <- 0

sensitivity(pred_log1, train$class)
specificity(pred_log1, train$class)
# biraz artÄ±ÅŸ olsa da yine de istediÄŸim oranÄ± saÄŸlayamadÄ±m

#Confusion Matrix
caret::confusionMatrix(as.factor(pred_log1), train$class)

#######              #############
####### test  verisi #############
#######       2      #############

pred_log = predict(logmodel1, newdata=test, type='response')
threshold <- 0.458937
pred_log[pred_log > threshold] <- 1
pred_log[pred_log < threshold] <- 0

sensitivity(pred_log, test$class)
specificity(pred_log, test$class)

# Confusion Matrix
caret::confusionMatrix(as.factor(pred_log), test$class)


#######              #############
####### train verisi #############
#######     3        #############
pred_log1 <- fitted(logmodel1)
summary(pred_log1)
threshold <- 0.80
pred_log1[pred_log1 > threshold] <- 1
pred_log1[pred_log1 < threshold] <- 0

sensitivity(pred_log1, train$class)
specificity(pred_log1, train$class)

#Confusion Matrix
caret::confusionMatrix(as.factor(pred_log1), train$class)

#######              #############
####### test  verisi #############
#######     3        #############
pred_log = predict(logmodel1, newdata=test, type='response')
summary(pred_log)
threshold <- 0.80
pred_log[pred_log > threshold] <- 1
pred_log[pred_log < threshold] <- 0

sensitivity(pred_log, test$class)
specificity(pred_log, test$class)

# Confusion Matrix
caret::confusionMatrix(as.factor(pred_log), test$class)



##############################################################################################################################################################################



str(df)
# veride 1, 4, 5, 8, 10, 12 degiskenleri numeric ve onlari LDA ve QDA da kullanacagiz.
train_num = train[,c(1,4,5,8,10,12,14)]
test_num = test[,c(1,4,5,8,10,12,14)]

###################################################
#-------------------------------------------------#
#          LINEAR DISCRIMINANT ANALYSIS           #
#-------------------------------------------------#
###################################################

# GruplarÄ±n ortalamalarÄ± arasÄ±nda maksimum ayrÄ±mÄ± yapacak ÅŸekilde bileÅŸen belirler.VarsayÄ±mlar: 


# â€¢ EÄŸer gruplar iyi ayrÄ±lmÄ±ÅŸ ise lojistik regresyona gÃ¶re daha duraÄŸan sonuÃ§ verir.
# â€¢ EÄŸer Ã¶rneklem bÃ¼yÃ¼klÃ¼ÄŸÃ¼ kÃ¼Ã§Ã¼k ve aÃ§Ä±klayÄ±cÄ± deÄŸiÅŸkenler normal daÄŸÄ±lÄ±ma sahip ise LDA daha iyi performans gÃ¶steriyor.
# â€¢ EÄŸer baÄŸÄ±mlÄ± deÄŸiÅŸken ikiden fazla sÄ±ralÄ± olmayan kategoriye sahip ise LDA daha iyi sonuÃ§ verir.
# â€¢ LDA ve lojistik regresyon yÃ¶ntemleri doÄŸrusal karar sÄ±nÄ±rlarÄ± verir.
# â€¢ EÄŸer gerÃ§ek sÄ±nÄ±rlar doÄŸrusal ise LDA ve Lojistik regresyon yÃ¶ntemleri iyi performans gÃ¶sterirler.

pairs.panels(train_num,
             gap=0,
             bg=c("red","blue")[train_num$class],
             pch=21)

desc=describeBy(train[1:13], train[,14])
desc

desc=describeBy(train[1:13], train[,12])
# her bir tÃ¼r iÃ§in bakmak skor hesaplamasÄ± iÃ§in gerekli
desc

model_lda<-lda(class~.,data = train_num)
model_lda
model_lda$prior
# %kaÃ§ deÄŸer aldÄ±klarÄ±nÄ±


pred_lda<-predict(model_lda,train_num)
hist_lda1<-ldahist(data=pred_lda$x[,1],g=train_num$class) 




pred_lda$class # tahmine gÃ¶re sÄ±nÄ±f ayÄ±rmasÄ±
pred_lda$posterior # gÃ¶zlemlerin gruplara dahil olma olasÄ±lÄ±klarÄ±nÄ± verir. birbirine yakÄ±n olasÄ±lÄ±klarÄ±n deÄŸeri Ã§ok fazla
pred_lda$x # fonksiyonda gÃ¶zlemlerin aldÄ±ÄŸÄ± deÄŸerleri gÃ¶sterir .histogram karÅŸÄ±laÅŸtÄ±rmasÄ±yla aynÄ± deÄŸerleri alÄ±r


str(train_num)
partimat(class ~., data=train_num,method="lda") #  kÄ±rmÄ±zÄ±lar yanlÄ±ÅŸ etiketleme gÃ¶sterir | iyi ayrÄ±ÅŸma yok
partimat(class ~ ., data = train_num, method = "lda", 
         plot.matrix = TRUE, imageplot = FALSE)


######

#Confusion matrix-accuracy-train
pred_lda<-predict(model_lda,train_num)
cfmatrix_1<-table(Tahmin=tahmin_1$class, Gercek=train_num$class) # sÄ±nÄ±flandÄ±rma sonuÃ§larÄ±
cfmatrix_1
accuracy_1<-sum(diag(cfmatrix_1))/sum(cfmatrix_1) # tahmin yÃ¼zdesini hesaplatÄ±r
accuracy_1


# %73 tahmin oranÄ±

#Confusion matrix-accuracy-test
tahmin_lda<-predict(model_lda,test_num)
cfmatrix_lda<-table(Tahmin=tahmin_lda$class, Gercek=test_num$class)
cfmatrix_lda
accuracy_lda<-sum(diag(cfmatrix_lda))/sum(cfmatrix_lda)
accuracy_lda

install.packages('ggord')
dev.off()

ggplot(df, aes(x = df$age, y = df$ca, col = class)) + 
  geom_point() +
  stat_ellipse() +
  scale_color_manual(values = c("blue", "red", "green"))






################################################################################################################################################





###################################################
#-------------------------------------------------#
#         QUADRATIC DISCRIMINANT ANALYSIS         #
#-------------------------------------------------#
###################################################

model_qda<-qda(class~.,data=train_num) 
model_qda
# Grup ortalamasindan en cok farka neden olan chol degiskeni gorunuyor

##################################
####### train verisi #############
#######              #############

tahmin_qda_1<-predict(model_qda,train_num)
cfmatrix_qda_1<-table(Tahmin=tahmin_qda_1$class, Gercek=train_num$class)

accuracy_qda_1<-mean(tahmin_qda_1$class==train_num$class)
accuracy_qda_1


##################################
####### test  verisi #############
#######              #############

tahmin_qda<-predict(model_qda , newdata=test_num )
cfmatrix_qda<-table(Tahmin=tahmin_qda$class, Gercek=test_num$class)

accuracy_qda<-mean(tahmin_qda$class==test_num$class)
accuracy_qda
#accuracy rate  biraz dustugunu goruyoruz

dev.off()
partimat(class~., data=train_num, method="qda")


sifir <- df[df$class==0,]
bir <- df[df$class==1,]

str(df)

HZ.test(sifir[,-c(2,7,3,6,9,13,11,14)])
HZ.test(bir[,-c(2,7,3,6,9,13,11,14)])




DH.test(sifir[,-c(2,7,3,6,9,13,11,14)])
DH.test(bir[,-c(2,7,3,6,9,13,11,14)])




sÄ±fÄ±r <- df[df$class==0, - c(2,7,3,6,9,13,11,14)] # data only for 0 class
res1 <- AD.test(sÄ±fÄ±r, qqplot = TRUE)
res1

# multivariate normal deÄŸil

bir <- df[df$class==1, - c(2,7,3,6,9,13,11,14)] # data only for 1 class
res2 <- AD.test(bir, qqplot = TRUE)
res2

# multivariate normal


### Varyans HomojenliÄŸi Testi

##Assumption Checking of LDA vs. QDA

str(df)


leveneTest(df$age ~ as.factor(df$class), df) # homojen 
leveneTest(df$trestbps ~ as.factor(df$class), df) # homojen
leveneTest(df$chol ~ as.factor(df$class), df) #homojen deÄŸil
leveneTest(df$thalach ~ as.factor(df$class), df) # homojen
leveneTest(df$oldpeak ~ as.factor(df$class), df)
leveneTest(df$ca ~ as.factor(df$class), df)



#We are using the BoxM test in order to check our assumption of homogeneity of variance-covariance matrices.
#H_o = Covariance matrices of the outcome variable are equal across all groups
#H_a = Covariance matrices of the outcome variable are different for at least one group






boxm <- heplots::boxM(df[, c(1,4,5,8,10,12)], df$class) 
boxm # p-value 0.05'ten kÃ¼Ã§Ã¼k Ã§Ä±ktÄ±ÄŸÄ± iÃ§in %95 gÃ¼venle baÄŸÄ±msÄ±z deÄŸiÅŸkenlerin kovaryans matrislerinin eÅŸit olduÄŸu varsayÄ±mÄ± reddedilemez
plot(boxm)



heplots::covEllipses(df[,c(1,4,5,8,10,12)], 
                     as.factor(df$class), 
                     fill = TRUE, 
                     pooled = FALSE, 
                     col = c("blue", "green"), 
                     variables = c(3,4), 
                     fill.alpha = 0.05) # deÄŸiÅŸkenlere gÃ¶re ne kadar ayrÄ±k olduÄŸu gÃ¶rÃ¼lÃ¼r. yuvarlaklar bÃ¼yÃ¼dÃ¼kÃ§e varyans fazlalÄ±k


#####



# NEURAL NETWORKS
# ----------------------------------------------------------------------------------------------- #

install.packages("neuralnet")
library("neuralnet")
str(df)

# neuralneti kullanabilmek icin tum veri numerik olmali + standardize edilmis olmali
df_dummy <- fastDummies::dummy_cols(df, select_columns = c("cp", "restecg", "slope", "thal"))

str(df)


# Verisetine kategorik degiskenlerin duzey sayisi - 1 kadar sutun eklemek amaciyla
# fazla sutunlar verisetinden cikarilir.

df_dummy <- subset(df_dummy, select = -c(cp, restecg, slope, thal, cp_4, restecg_2, slope_3, thal_7))
df_dummy <- df_dummy[,c(1,2,3,4,5,6,7,8,9,11,12,13,14,15,16,17,18,19,10)] # changing columns sort

df_dummy$cp_1 <- as.factor(df_dummy$cp_1)
df_dummy$cp_2 <- as.factor(df_dummy$cp_2)
df_dummy$cp_3 <- as.factor(df_dummy$cp_3)

df_dummy$restecg_0 <- as.factor(df_dummy$restecg_0)
df_dummy$restecg_1 <- as.factor(df_dummy$restecg_1)

df_dummy$slope_1 <- as.factor(df_dummy$slope_1)
df_dummy$slope_2 <- as.factor(df_dummy$slope_2)

df_dummy$thal_3 <- as.factor(df_dummy$thal_3)
df_dummy$thal_6 <- as.factor(df_dummy$thal_6)

str(df_dummy)

# Ortalamalarla Veri Standardizasyonu
library(PreProcess)
str(nndata)


scaleModel <- caret::preProcess(df_dummy, method=c("center", "scale"))

nndata <- predict(scaleModel, df_dummy)

nndata$sex <- as.numeric(nndata$sex)
nndata$sex[nndata$sex == 1] <- 0
nnData$sex[nndata$sex == 2] <- 1

nndata$fbs <- as.numeric(nndata$fbs)
nndata$fbs[nndata$fbs == 1] <- 0
nndata$fbs[nndata$fbs == 2] <- 1

nndata$exang <- as.numeric(nndata$exang)
nndata$exang[nndata$exang == 1] <- 0
nndata$exang[nndata$exang == 2] <- 1

nndata$class <- as.numeric(nndata$class)
nndata$class[nndata$class == 1] <- 0
nndata$class[nndata$class == 2] <- 1

nndata$cp_1 <- as.numeric(nndata$cp_1)
nndata$cp_1[nndata$cp_1 == 1] <- 0
nndata$cp_1[nndata$cp_1 == 2] <- 1

nndata$cp_2 <- as.numeric(nndata$cp_2)
nndata$cp_2[nndata$cp_2 == 1] <- 0
nndata$cp_2[nndata$cp_2 == 2] <- 1

nndata$cp_3 <- as.numeric(nndata$cp_3)
nndata$cp_3[nndata$cp_3 == 1] <- 0
nndata$cp_3[nndata$cp_3 == 2] <- 1

nndata$restecg_0 <- as.numeric(nndata$restecg_0)
nndata$restecg_0[nndata$restecg_0 == 1] <- 0
nndata$restecg_0[nndata$restecg_0 == 2] <- 1

nndata$restecg_1 <- as.numeric(nndata$restecg_1)
nndata$restecg_1[nndata$restecg_1 == 1] <- 0
nndata$restecg_1[nndata$restecg_1 == 2] <- 1

nndata$slope_1 <- as.numeric(nndata$slope_1)
nndata$slope_1[nndata$slope_1 == 1] <- 0
nndata$slope_1[nndata$slope_1 == 2] <- 1

nndata$slope_2 <- as.numeric(nndata$slope_2)
nndata$slope_2[nndata$slope_2 == 1] <- 0
nndata$slope_2[nndata$slope_2 == 2] <- 1

nndata$thal_3 <- as.numeric(nndata$thal_3)
nndata$thal_3[nndata$thal_3 == 1] <- 0
nndata$thal_3[nndata$thal_3 == 2] <- 1

nndata$thal_6 <- as.numeric(nndata$thal_6)
nndata$thal_6[nndata$thal_6 == 1] <- 0
nndata$thal_6[nndata$thal_6 == 2] <- 1

str(nndata)

nn_traindata <- nndata[train_ind, ]
nn_testdata <- nndata[-train_ind, ]


dim(nn_traindata)
dim(nn_testdata)

str(nn_traindata)


modelNN1 <- neuralnet(class ~ ., data=nn_traindata,
                      hidden=4, threshold=0.01,
                      learningrate = 0.05,
                      linear.output = FALSE,
                      err.fct="sse")

plot(modelNN1)

fitted_values_1 <- modelNN1$net.result[[1]]

# test verisi
pred_test_1 <- predict(modelNN1, newdata=nn_testdata)

pred_test_1[pred_test_1 >= 0.50] <- 1
pred_test_1[pred_test_1 < 0.50] <- 0
pred_test_1 <- as.factor(pred_test_1)

table_nn <- table(as.factor(pred_test_1), nn_testdata$class)
table_nn

accuracy_nn <- sum(diag(table_nn))/sum(table_nn)
accuracy_nn

caret::confusionMatrix(pred_test_1, as.factor(nn_testdata$class))





##############################################################################################################################################################





roc_ct  <- roc(test$class ~ as.numeric(levels(tree_pred))[tree_pred],levels = c(0, 1), direction = "<")
roc_bag <- roc(test$class ~ as.numeric(levels(baggintest))[baggintest] , levels =c(0,1), direction="<")
roc_rf  <- roc(test$class ~ as.numeric(levels(rf_tahmin))[rf_tahmin], levels =c(0,1), direction="<" )
roc_log <- roc(test$class ~ pred_log, levels =c(0,1), direction="<")
roc_lda <- roc(test_num$class ~ as.numeric(levels(tahmin_lda$class))[tahmin_lda$class], levels =c(0,1), direction="<" )
roc_qda <- roc(test_num$class ~ as.numeric(levels(tahmin_qda$class))[tahmin_qda$class],  levels =c(0,1), direction="<" )


lwd = 3
plot(roc_ct, color ='green',lwd=lwd,main="ROC Curve")
plot(roc_bag, add=TRUE, col='red',lwd=lwd) 
plot(roc_rf, add=TRUE, col='blue',lwd=lwd)
plot(roc_log, add=TRUE, col='purple2',lwd=lwd)
plot(roc_lda, add=TRUE, col='brown',lwd=lwd)
plot(roc_qda, add=TRUE, col='yellow3',lwd=lwd)


auc(roc_ct)
auc(roc_bag)
auc(roc_rf)
auc(roc_log)
auc(roc_lda)
auc(roc_qda)



